
### 梯度下降法
原文链接：
https://blog.csdn.net/ayangann915/article/details/81097530

```python
import numpy as np
import matplotlib.pyplot as plt
```


```python
plot_x=np.linspace(-1.,6.,141)
plot_y=(plot_x-2.5)**2-1.
```


```python
plt.plot(plot_x,plot_y)
plt.show()
```


![png](output_3_0.png)


#### 进行梯度下降，寻找最小值

这里我们设置步长为0.1，设置差异最小值为1e-8


```python
eta=0.1#学习率，即步长
esilon=1e-8
```


```python
#定义损失函数
def J(theta):
    return (theta-2.5)**2-1.

#对损失函数求导
def dJ(theta):
    return 2*(theta-2.5)

#寻找最小值
theta=0.0
while True:
    gradient=dJ(theta)#求出当前点对应的梯度
    last_theta=theta
    theta=theta-eta*gradient#向导数的负方向移动
    
    if(abs(J(theta)-J(last_theta))<esilon):#梯度下降，新得出的损失函数值应该比原先的损失函数值要小，如果差值小于epsilon，可认为到达最小值
        break#满足条件则退出循环

print(theta)#最小值时theta的取值
print(J(theta))#损失函数的最小值
```

    2.499891109642585
    -0.99999998814289


#### 将原曲线和梯度下降取值的过程绘制出来
我们将寻找最小值过程中经过的每一个theta值保存下来，作为绘制梯度下降过程的点


```python
theta_history=[]#存储梯度下降过程中的theta值
def J(theta):
    return (theta-2.5)**2-1.

def dJ(theta):
    return 2*(theta-2.5)

theta=0.0
while True:
    gradient=dJ(theta)
    last_theta=theta
    theta=theta-eta*gradient
    theta_history.append(theta)
    
    if(abs(J(theta)-J(last_theta)))<esilon:
        break

plt.plot(plot_x,plot_y)
plt.plot(np.array(theta_history),J(np.array(theta_history)),color="r",marker="+")
plt.show()
```


![png](output_9_0.png)



```python
#计算经过了几次查找
len(theta_history)
```




    45



#### 封装梯度下降的过程


```python
theta_history=[]
def gradient_decent(theta,eta,epsilon=1e-8):
   #last_theta=theta
    #theta_history.append(theta)
    
    while True:
        gradient=dJ(theta)
        last_theta=theta
        theta=theta-eta*gradient
        theta_history.append(theta)
        
        if(abs(J(theta)-J(last_theta))<epsilon):
            break

def plot_theta():
    plt.plot(plot_x,J(plot_x))
    plt.plot(np.array(theta_history),J(np.array(theta_history)),color="r",marker="+")     
```


```python
eta=0.01
theta_history=[]
gradient_decent(0.,eta)
plot_theta()
```


![png](output_13_0.png)



```python
len(theta_history)
```




    423




```python
eta=0.001
theta_history=[]
gradient_decent(0.,eta)
plot_theta()
```


![png](output_15_0.png)



```python
len(theta_history)
```




    3681




```python
eta=0.8
theta_history=[]
gradient_decent(0.,eta)
plot_theta()
```


![png](output_17_0.png)



```python
len(theta_history)
```




    21




```python
eta=1.1
theta_history=[]
gradient_decent(0.,eta)
plot_theta()
```


    ---------------------------------------------------------------------------

    OverflowError                             Traceback (most recent call last)

    <ipython-input-16-0fb9d6f19963> in <module>()
          1 eta=1.1
          2 theta_history=[]
    ----> 3 gradient_decent(0.,eta)
          4 plot_theta()


    <ipython-input-9-d6d6057f09e4> in gradient_decent(theta, eta, epsilon)
         10         theta_history.append(theta)
         11 
    ---> 12         if(abs(J(theta)-J(last_theta))<epsilon):
         13             break
         14 


    <ipython-input-7-dcddad55213a> in J(theta)
          1 theta_history=[]#存储梯度下降过程中的theta值
          2 def J(theta):
    ----> 3     return (theta-2.5)**2-1.
          4 
          5 def dJ(theta):


    OverflowError: (34, 'Result too large')


#### 异常解析
eta=1.1时，由于eta值过大，导致梯度下降过程中损失函数对应的值越来越大，最后超过了可以计算的量程，因此，要对损失函数进行异常处理


```python
def J(theta):
    try:
        return (theta-2.5)**2-1.
    except:
        return float('inf')#这是一个无限大的浮点数
```


```python
eta=1.1
theta_history=[]
gradient_decent(0.,eta)
plot_theta()
```

由上可知，并没有打印出图像，函数是一个死循环，因为始终找不到合适的最小值的点，因此我们对gradient_decent(）函数进行改进，给他设置一个循环的最大次数


```python
theta_history=[]
def gradient_decent(theta,eta,n_iters=1e4,epsilon=1e-8):  
    i=0
    while i<n_iters:
        gradient=dJ(theta)
        last_theta=theta
        theta=theta-eta*gradient
        theta_history.append(theta)
        
        if(abs(J(theta)-J(last_theta))<epsilon):
            break
        i+=1
```
