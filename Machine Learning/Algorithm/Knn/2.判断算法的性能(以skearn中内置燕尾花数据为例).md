* 一般通过训练数据训练出算法模型，但是要测试出算法的精确度时，通常要用真实生活中的数据，但是真实数据不易获得，因此我们要采取办法来解决它。
* 一般而言，训练数据具有一定的真实性，在实际数据不易获得的情况下，我们可以将训练数据划分为两部分，一部分作为训练数据，另一部分作为测试数据
* 这样测试数据也就具有一定的真实性了

下面我们通过一个实例来测试如此方法下算法的性能（仍然以燕尾花的数据为例）
```python
import numpy as np

import matplotlib.pyplot as plt

from sklearn import datasets

iris=datasets.load_iris

iris=datasets.load_iris()

iris.keys()
Out[25]: dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])

X=iris.data

y=iris.target

y
Out[28]: 
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])


#测试函数，传入参数为数据X，类别y，测试数据所占比例，随机种子
def train_test_split(X, y, test_ratio=0.2, seed=None):
    if seed:
        np.random.seed(seed)
        
        """打乱数据,
shuffle与permutation的区别:
函数shuffle与permutation都是对原来的数组进行重新洗牌（即随机打乱原来的元素顺序）；
区别在于shuffle直接在原来的数组上进行操作，改变原来数组的顺序，无返回值。
而permutation不直接在原来的数组上进行操作，而是返回一个新的打乱顺序的数组，并不改变原来的数组。"""

    shuffle_indexes=np.random.permutation(len(X))
    text_size=int(len(X)*text_radio)#测试数据的长度
    text_index=shuffle_indexes[:text_size]#测试数据
    train_index=shuffle_indexes[text_size:]#训练数据
    
    #使用fancing Indexes，传入索引的数组
    X_train=X[train_index]
    y_train=y[train_index]
    
    X_text=X[text_index]
    y_text=y[text_index]
    
    return X_train,y_train,X_text,y_text;
    
#测试    
X_train,y_train,X_text,y_text=train_test_split(X,y,0.2,seed=666)

#打印数组大小
print(X_train.shape)
print(y_train.shape)

print(X_text.shape)
print(y_text.shape)

######################
结果为：
(120, 4)
(120,)
(30, 4)
(30,)
In [149]:
######################

knn=KNeighborsClassifier(3)#这里使用的是所写的knn算法，见knn第一部分
knn.fit(X_train,y_train)#拟合数据
y_predict=knn.predict(X_text)#预测数据

y_predict
#预测结果：array([1, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 2, 1, 0, 2, 2, 2, 1, 0,
       2, 0, 1, 1, 0, 1, 2, 2])

#预测结果和测试部分y相等的数量：30
sum(y_predict==y_text)

#相等率：1.0
sum(y_predict==y_text)/len(y_text)
```
由此可见，性能较好，但是和实际生活中还是有差距的
